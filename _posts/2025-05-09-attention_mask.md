---
layout: post
title: attention_mask
subtitle: ğŸ«¥ encoder- and decoder-only standard attention_mask
tags: [huggingface]
gh-repo: Peg-Wu
gh-badge: [star, fork, follow]
cover-img: /assets/img/blog_imgs/2025-05-09-attention_mask/cover.png
comments: true
mathjax: true
author: Pengpeng Wu
---

{: .box-success}
ğŸš… çœ‹äº†ä¸€ä¸‹huggingfaceå®˜æ–¹æ˜¯æ€ä¹ˆå†™æ ‡å‡†çš„encoder-onlyå’Œdecoder-onlyçš„attention_maskçš„ï¼Œå†™çš„è¿˜æ˜¯éå¸¸æœ‰æ„æ€çš„~

- transformersï¼š4.51.3



- æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œtransformersçš„attention_maskåœ¨ä¼ å…¥æ¨¡å‹å‰æ˜¯äºŒç»´çš„å¼ é‡ï¼Œå½¢çŠ¶ä¸ºbatch_size x seq_lenï¼Œå¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œ**<u>æœ‰æ•ˆtokençš„ä½ç½®æ˜¯1ï¼Œå¡«å……tokençš„ä½ç½®æ˜¯0</u>**
- å¯¹attention_maskçš„è¿›ä¸€æ­¥å¤„ç†åœ¨æ¨¡å‹çš„forwardé˜¶æ®µï¼Œ**<u>ä¸€èˆ¬å†™åœ¨æ¨¡å‹çš„backboneä¸­</u>**ï¼Œä¸‹é¢æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä»£ç ï¼š
- ä»£ç ä½ç½®ï¼š**transformers.modeling_utils.ModuleUtilsMixin.get_extended_attention_mask**ï¼ŒPreTrainedModelç»§æ‰¿äº†ModuleUtilsMixinç±»ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥åœ¨æ¨¡å‹çš„forwardä¸­ä½¿ç”¨get_extended_attention_maskå‡½æ•°

```python
import torch

"""encoder-only mask"""
# (1, 5) -> (batch_size, seq_len)
attention_mask = torch.tensor([[1, 1, 1, 0, 0]])

# (1, 1, 1, 5) -> (batch_size, 1, 1, seq_len)
attention_mask = attention_mask[:, None, None, :]

# 1 -> 0; 0 -> -inf
attention_mask = (1 - attention_mask) * torch.finfo(type=torch.float32).min
attention_mask  # (batch_size, 1, 1, seq_len)

"""
Output:
tensor([[[[-0.0000e+00, -0.0000e+00, -0.0000e+00, -3.4028e+38, -3.4028e+38]]]])
"""


"""decoder-only mask"""
# (1, 5) -> (batch_size, seq_len)
attention_mask = torch.tensor([[1, 1, 1, 0, 0]])

batch_size, seq_len = attention_mask.shape

seq_ids = torch.arange(seq_len)

# (1, 5, 5) -> (batch_size, seq_len, seq_len)
causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_len, 1) <= seq_ids[None, :, None]

# (1, 1, 5, 5) -> (batch_size, 1, seq_len, seq_len)
attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]

# 1 -> 0; 0 -> -inf
attention_mask = (1 - attention_mask) * torch.finfo(type=torch.float32).min
attention_mask  # (batch_size, 1, seq_len, seq_len)

"""
Output:
tensor([[[[-0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38],
          [-0.0000e+00, -0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],
          [-0.0000e+00, -0.0000e+00, -0.0000e+00, -3.4028e+38, -3.4028e+38],
          [-0.0000e+00, -0.0000e+00, -0.0000e+00, -3.4028e+38, -3.4028e+38],
          [-0.0000e+00, -0.0000e+00, -0.0000e+00, -3.4028e+38, -3.4028e+38]]]])
"""
```

- ä¸è®ºæ˜¯encoder-onlyè¿˜æ˜¯decoder-onlyçš„æ¨¡å‹ï¼Œæœ€ç»ˆç”Ÿæˆçš„attention_maskéƒ½æ˜¯4ç»´çš„å¼ é‡ï¼Œå…¶ä¸­batch_sizeåé¢çš„1ç»´å…¶å®æ˜¯ç»™nheadsé¢„ç•™çš„ï¼Œåé¢ä¼šé€šè¿‡å¹¿æ’­æœºåˆ¶å°†attention_maskä¸attention_weight**<u>ç›¸åŠ </u>**ï¼ˆqueryå’Œkeyç‚¹ç§¯å¹¶ç¼©æ”¾åï¼Œsoftmaxå‰çš„attention_weightï¼Œå½¢çŠ¶æ˜¯batch_size x nheads x seq_len x seq_lenï¼‰
- attention_maskä¸­0çš„éƒ¨åˆ†è¡¨ç¤ºä¸è¢«æ©ç ï¼Œè´Ÿæ— ç©·çš„éƒ¨åˆ†è¡¨ç¤ºè¢«æ©ç 
- ğŸ‘¾ æ€»ä½“ä¸Šå†™çš„è¿˜æ˜¯éå¸¸æœ‰æ„æ€çš„ï¼ŒåŒ…å«å¾ˆå¤šå¢åŠ ç»´åº¦å’Œå¹¿æ’­è¿ç®—çš„æ“ä½œï¼Œå€¼å¾—åå¤è§‚çœ‹~