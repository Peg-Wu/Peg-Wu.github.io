---
layout: post
title: TensorParallel
subtitle: ğŸŒ» ç¾ä¸½çš„èŠ±è™½ç„¶ä¼šå‡‹è°¢ï¼Œå¯æ˜¯ç››å¼€çš„æ—¶åˆ»å€¼å¾—æ¬£èµ ~
tags: [huggingface, TP]
gh-repo: Peg-Wu
gh-badge: [star, fork, follow]
cover-img: /assets/img/blog_imgs/2025-04-14-TensorParallel/cover.png
comments: true
mathjax: true
author: Pengpeng Wu
---

{: .box-success}
ğŸ¤– æœ¬èŠ‚å°†è§£è¯»Huggingfaceæ˜¯å¦‚ä½•å®ç°å¼ é‡å¹¶è¡Œ(TP)çš„~

## 1. TP ç®€ä»‹

- åœ¨å¼ é‡å¹¶è¡Œä¸­ï¼Œçº¿æ€§å±‚çš„è®¡ç®—å¯ä»¥åœ¨GPUä¹‹é—´è¿›è¡Œæ‹†åˆ†ã€‚è¿™æ ·å¯ä»¥èŠ‚çœå†…å­˜ï¼Œå› ä¸ºæ¯ä¸ªGPUåªéœ€è¦ä¿å­˜æƒé‡çŸ©é˜µçš„ä¸€éƒ¨åˆ†ã€‚çº¿æ€§å±‚æœ‰ä¸¤ç§æ‹†åˆ†æ–¹å¼ï¼šæŒ‰è¡Œå’ŒæŒ‰åˆ—ã€‚

### 1.1 Column-wise Parallel

- åœ¨Column-wise Parallelä¸­ï¼Œæƒé‡çŸ©é˜µæŒ‰åˆ—ç»´åº¦å‡åŒ€åˆ†å‰²ã€‚æ¯ä¸ªGPUéƒ½æ”¶åˆ°ç›¸åŒçš„è¾“å…¥ï¼Œå¹¶ä½¿ç”¨å…¶æƒé‡çŸ©é˜µéƒ¨åˆ†è®¡ç®—å¸¸è§„çŸ©é˜µä¹˜æ³•ã€‚æœ€åï¼Œæ¯ä¸ªGPUçš„è¾“å‡ºconcatèµ·æ¥å½¢æˆæœ€ç»ˆè¾“å‡ºã€‚

![tp-colwise](/assets/img/blog_imgs/2025-04-14-TensorParallel/tp-colwise.jpeg)

### 1.2 Row-wise Parallel

- åœ¨Row-wise Parallelä¸­ï¼Œæƒé‡çŸ©é˜µæŒ‰è¡Œç»´åº¦å‡åŒ€åˆ†å‰²ï¼Œæ­¤å¤–ï¼Œè¾“å…¥çŸ©é˜µä¹Ÿè¦æŒ‰ç…§åˆ—ç»´åº¦å‡åŒ€åˆ†å‰²ã€‚æ¯ä¸ªGPUæ”¶åˆ°å¯¹åº”çš„è¾“å…¥å’Œæƒé‡çŸ©é˜µï¼Œå¹¶è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚æœ€åï¼Œæ¯ä¸ªGPUçš„è¾“å‡ºç›¸åŠ å½¢æˆæœ€ç»ˆè¾“å‡ºã€‚

![tp-rowwise](/assets/img/blog_imgs/2025-04-14-TensorParallel/tp-rowwise.jpeg)

### 1.3 Combined Column- and Row-wise Parallel

- Column-wise Parallel å’Œ Row-wise Parallel ä¸»è¦åº”ç”¨äº Y=XW è¿™ç§åªæœ‰ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜çš„æƒ…å†µï¼Œå½“æœ‰3ä¸ªæˆ–å¤šä¸ªçŸ©é˜µç›¸ä¹˜æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç»“åˆè¿™ä¸¤ç§åˆ†å‰²æ–¹å¼ä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚

![tp-combined.jpeg](/assets/img/blog_imgs/2025-04-14-TensorParallel/tp-combined.jpeg)

## 2. ä»£ç å®ç°

- åœ¨LLMä¸­ï¼Œé€šå¸¸ä¼šä¸ºMLPå±‚å’ŒAttentionå±‚å®ç°TP

- ä½†æ˜¯ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œhuggingfaceçš„transformerså®ç°çš„TPï¼Œåªæ˜¯é™ä½äº†å•ä¸ªçŸ©é˜µè®¡ç®—çš„å¤§å°ï¼Œå¹¶æ²¡æœ‰æŠŠå„ä¸ªçŸ©é˜µåˆ†ç‰‡åˆ°ä¸åŒçš„æ˜¾å¡ä¸Šï¼Œå¦‚æœéœ€è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦æ”¹ä¸€ä¸‹ä»£ç ï¼Œåˆ†é…æƒé‡æ‰€åœ¨è®¾å¤‡

### 2.1 MLP

```python
class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        if self.config.pretraining_tp > 1:
            slice = self.intermediate_size // self.config.pretraining_tp
            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)
            up_proj_slices = self.up_proj.weight.split(slice, dim=0)
            down_proj_slices = self.down_proj.weight.split(slice, dim=1)

            gate_proj = torch.cat(
                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1
            )
            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)

            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)
            down_proj = [
                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)
            ]
            down_proj = sum(down_proj)
        else:
            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

        return down_proj
```

- åœ¨LlamaMLPä¸­ï¼Œå®˜æ–¹å®ç°æ˜¯å¯¹gate_projå’Œup_projçš„æƒé‡è¿›è¡Œäº†åˆ—åˆ†å‰²ï¼Œå¯¹down_projçš„æƒé‡è¿›è¡Œäº†è¡Œåˆ†å‰²ï¼Œå¹¶æœªé‡‡ç”¨æ··åˆåˆ†å‰²çš„æ–¹å¼ã€‚

### 2.3 Attention

```python
class LlamaAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: LlamaConfig):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self._init_rope()

    def _init_rope(self):
        if self.config.rope_scaling is None:
            self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)
        else:
            scaling_type = self.config.rope_scaling["type"]
            scaling_factor = self.config.rope_scaling["factor"]
            if scaling_type == "linear":
                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(
                    self.head_dim, max_position_embeddings=self.max_position_embeddings, scaling_factor=scaling_factor
                )
            elif scaling_type == "dynamic":
                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(
                    self.head_dim, max_position_embeddings=self.max_position_embeddings, scaling_factor=scaling_factor
                )
            else:
                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")

    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        bsz, q_len, _ = hidden_states.size()

        if self.config.pretraining_tp > 1:
            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp
            query_slices = self.q_proj.weight.split(
                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0
            )
            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)
            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)

            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]
            query_states = torch.cat(query_states, dim=-1)

            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]
            key_states = torch.cat(key_states, dim=-1)

            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]
            value_states = torch.cat(value_states, dim=-1)

        else:
            query_states = self.q_proj(hidden_states)
            key_states = self.k_proj(hidden_states)
            value_states = self.v_proj(hidden_states)

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            kv_seq_len += past_key_value[0].shape[-2]
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            # reuse k, v, self_attention
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)

        past_key_value = (key_states, value_states) if use_cache else None

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                )
            attn_weights = attn_weights + attention_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

        if self.config.pretraining_tp > 1:
            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)
            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)
            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])
        else:
            attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value
```

- åœ¨LlamaAttentionä¸­ï¼Œå®ç°TPçš„éƒ¨åˆ†ä¸»è¦åŒ…å«ï¼š

1. ç”ŸæˆQKVçŸ©é˜µæ—¶ï¼šå‡å¯¹æƒé‡çŸ©é˜µé‡‡å–åˆ—åˆ†å‰²

2. æœ€åä¸€ä¸ªå…¨è¿æ¥è¾“å‡ºå±‚ï¼šé‡‡ç”¨è¡Œåˆ†å‰²ï¼Œå¯¹attn_outputè¿›è¡Œåˆ—åˆ†å‰²ï¼Œå¯¹æƒé‡çŸ©é˜µè¿›è¡Œè¡Œåˆ†å‰²

{: .box-note}
**Note:** éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œnn.Linear(a, b)ï¼Œå…¶å¯¹åº”çš„æƒé‡çŸ©é˜µå½¢çŠ¶æ˜¯(b, a), åœ¨è¿›è¡Œåˆ—åˆ†å‰²æ—¶ï¼Œæˆ‘ä»¬éœ€è¦åœ¨bç»´åº¦(dim=0)è¿›è¡Œåˆ†å‰²ï¼›åœ¨è¿›è¡Œè¡Œåˆ†å‰²æ—¶ï¼Œæˆ‘ä»¬éœ€è¦åœ¨aç»´åº¦(dim=-1)è¿›è¡Œåˆ†å‰²ï¼Œä¸è¦æé”™åˆ†å‰²çš„ç»´åº¦~ğŸ¤—