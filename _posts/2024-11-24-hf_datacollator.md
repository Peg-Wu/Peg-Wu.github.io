---
layout: post
title: Huggingface DataCollator
subtitle: ðŸ˜Ž æ¬²ä¹°æ¡‚èŠ±åŒè½½é…’ï¼Œç»ˆä¸ä¼¼ï¼Œå°‘å¹´æ¸¸ã€‚
tags: [huggingface, datacollator]
gh-repo: Peg-Wu
gh-badge: [star, fork, follow]
cover-img: /assets/img/blog_imgs/2024-11-24-hf_datacollator/cover.png
comments: true
mathjax: true
author: Pengpeng Wu
---

{: .box-success}
æœ€è¿‘é‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œåšnlpçš„æ—¶å€™å¾ˆå¤štokenizeréƒ½æ˜¯ç›´æŽ¥ä»Žhuggingface hubä¸Šæ‹‰ä¸‹æ¥çš„æˆ–è€…ç”¨è‡ªå·±çš„æ–‡æœ¬è¯­æ–™è®­ç»ƒå‡ºæ¥çš„ã€‚ä½†æ˜¯åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸï¼Œä»¥å•ç»†èƒžè½¬å½•ç»„å¤§æ¨¡åž‹geneformerä¸ºä¾‹ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªç»†èƒžï¼Œæ¯ä¸ªtokenæ˜¯ä¸€ä¸ªgeneï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦è¿›è¡Œåˆ†è¯æ“ä½œï¼ˆç›¸å½“äºŽå·²ç»åˆ†å¥½è¯äº†ï¼‰ï¼Œæˆ‘ä»¬å®Œå…¨å¯ä»¥è‡ªå·±å®šä¹‰ä¸€ä¸ªTokenizerç±»ï¼Œç”¨äºŽå°†geneè½¬æ¢æˆidï¼Œå¹¶è¿›è¡Œæˆªæ–­ï¼Œå¡«å……ï¼ŒåŠ ç‰¹æ®Šå­—ç¬¦ç­‰æ“ä½œã€‚ç”±äºŽæˆ‘åŽç»­æƒ³è¦ä½¿ç”¨transformersä¸­çš„DataCollatorForLanguageModelingï¼Œä½†æ˜¯å®žä¾‹åŒ–è¿™ä¸ªç±»æ—¶å¿…é¡»è¦ä¼ å…¥ä¸€ä¸ªtokenizerï¼Œå¦‚æžœæˆ‘æƒ³è¦ä½¿ç”¨è¿™ä¸ªdatacollatorï¼Œæˆ‘å¿…é¡»æžæ¸…æ¥šå®ƒä½¿ç”¨äº†tokenizerä¸­çš„å“ªäº›å±žæ€§å’Œæ–¹æ³•ï¼Œç„¶åŽè‡ªå®šä¹‰ä¸€ä¸ª"ä¼ª"tokenizeræˆ–è€…å°†å¿…è¦çš„å±žæ€§å’Œæ–¹æ³•èžå…¥æˆ‘è‡ªå®šä¹‰çš„Tokenizerç±»ï¼Œä½¿ä¸¤è€…èƒ½å¤Ÿç›¸äº’é€‚é…~ï¼ˆä¸»è¦æ˜¯å› ä¸ºå¤ªæ‡’äº†ï¼Œä¸æƒ³è‡ªå·±é‡æ–°å†™ä¸€ä¸ªdatacollatorï¼Œè€Œä¸”è¿™æ ·åšæ•ˆçŽ‡ä¸ä¸€å®šæ¯”å®˜æ–¹å®žçŽ°çš„é«˜ï¼Œä¸ä¼˜é›…ï¼‰ðŸ¤·â€â™‚ï¸

**åŽé¢æˆ‘å°±æŽ¢ç´¢äº†ä¸€ä¸‹DataCollatorWithPaddingå’ŒDataCollatorForLanguageModelingï¼Œè¿™ä¸¤ä¸ªä¹Ÿæ˜¯å¹³æ—¶æœ€å¸¸ä½¿ç”¨çš„ä¸¤ä¸ªdata_collator**

- æ•°æ®é›†ä¸‹è½½é“¾æŽ¥ï¼š[download_dataset](https://github.com/zyds/transformers-code/tree/master/02-NLP%20Tasks/14-language_model/wiki_cn_filtered)

- DEBUGï¼š**(transformers == 4.46.3)**

```python
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HOME"] = "~/hf"

from datasets import load_from_disk
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling


# load and preprocess dataset
ds = load_from_disk("./data/wiki_cn_filtered/")
tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-macbert-base")

def process_func(examples):
    return tokenizer(examples["completion"], max_length=384, truncation=True)

tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds.column_names)


# dataloader, using different data_collator
dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorWithPadding(tokenizer))
# dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=0.15))

next(iter(dl))
```

## 1. DataCollatorWithPadding

![datacollatorwithpadding.drawio](/assets/img/blog_imgs/2024-11-24-hf_datacollator/datacollatorwithpadding.drawio.png){: .mx-auto.d-block :}

{: .box-note}
**Note:** æ ¹æ®ä»¥ä¸Šæºç é€»è¾‘ï¼Œæˆ‘ä»¬ä¼¼ä¹Žåªéœ€è¦ç»§æ‰¿`PretrainedTokenizerBase`ç±»ï¼Œè‡ªå®šä¹‰å¿…è¦å±žæ€§å³å¯ï¼Œå…·ä½“çš„å‡½æ•°å®žçŽ°æ— éœ€ä¿®æ”¹ï¼Œå…¶ä¸­ï¼š`pad_token`å’Œ`pad_token_id`å±žæ€§æ˜¯å¿…é¡»è¦æ·»åŠ çš„ï¼Œ`pad_token`å¯ä»¥é€šè¿‡è°ƒç”¨`SpecialTokensMixin`çš„initæ–¹æ³•å¸®åŠ©æˆ‘ä»¬æ·»åŠ ï¼Œ`pad_token_id`å¯ä»¥é€šè¿‡å®šä¹‰`convert_tokens_to_ids`æ–¹æ³•éšå¼æ·»åŠ ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨ç±»å±žæ€§ä¸­è®¾ç½®`padding_side`ï¼Œç¡®å®šå¡«å……çš„ä½ç½®ï¼Œè¯ä¸å¤šè¯´ï¼Œç›´æŽ¥ä¸Šä»£ç ðŸ¥±~

### 1.1 æš´åŠ›ä¿®æ”¹

```python
from transformers import PreTrainedTokenizerBase, DataCollatorWithPadding

class PreCollator(PreTrainedTokenizerBase):
    pad_token = ["PAD"]
    pad_token_id = 0
    padding_side = "right"

precollator = PreCollator()
data_collator = DataCollatorWithPadding(precollator)


samples = [{"input_ids": [1, 2, 3], "token_type_ids": [0, 0, 0], "attention_mask": [1, 1, 1], "labels": 1},
           {"input_ids": [4, 5], "token_type_ids": [0, 0], "attention_mask": [1, 1], "labels": 0}]

data_collator(samples)
```

### 1.2 æ›´åŠ ä¼˜é›…çš„æ–¹å¼

```python
from typing import Union, List
from transformers import PreTrainedTokenizerBase, DataCollatorWithPadding

vocab = {
    "[PAD]": 0,
    "[UNK]": 1,
    "[CLS]": 2,
    "[SEP]": 3,
    "[MASK]": 4,
    # ...
}

class PreCollator(PreTrainedTokenizerBase):

    padding_side = "right"

    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:
        """
        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
        vocabulary.

        Args:
            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).

        Returns:
            `int` or `List[int]`: The token id or list of token ids.
        """
        if tokens is None:
            return None

        if isinstance(tokens, str):
            return self._convert_token_to_id_with_added_voc(tokens)

        ids = []
        for token in tokens:
            ids.append(self._convert_token_to_id_with_added_voc(token))
        return ids
    

    def _convert_token_to_id_with_added_voc(self, token):
        if token is None:
            return None

        return vocab.get(token)
    

precollator = PreCollator(pad_token="[PAD]")
data_collator = DataCollatorWithPadding(precollator)


samples = [{"input_ids": [1, 2, 3], "token_type_ids": [0, 0, 0], "attention_mask": [1, 1, 1], "labels": 1},
           {"input_ids": [4, 5], "token_type_ids": [0, 0], "attention_mask": [1, 1], "labels": 0}]

data_collator(samples)
```

ðŸ˜‚ è™½ç„¶ä¼˜é›…ï¼Œä½†æ˜¯éœ€è¦å¯¹æºç æœ‰å……åˆ†çš„ç†è§£ ~

## 2. DataColloatorForLanguageModeling

åœ¨æžæ¸…æ¥šDataCollatorWithPaddingçš„æºç é€»è¾‘åŽï¼Œå†çœ‹DataCollatorForLanguageModelingï¼Œæ•´ä¸ªäººé†é†çŒé¡¶ï¼ˆbushiï¼‰

![image-20241124205739063]((/assets/img/blog_imgs/2024-11-24-hf_datacollator/1.png){: .mx-auto.d-block :}

**DataCollatorForLanguageModelingä¸å°±æ˜¯å…ˆæ‰§è¡Œäº†ä¸€ä¸‹DataCollatorWithPaddingçš„é€»è¾‘ï¼Œç„¶åŽåšäº†ä¸€äº›åŽå¤„ç†ï¼Ÿ**

- å…ˆçœ‹ä¸€ä¸‹clmä»»åŠ¡çš„é€»è¾‘ï¼šå°±æ˜¯æŠŠpaddingåŽçš„input_idså…‹éš†äº†ä¸€ä»½ï¼Œç„¶åŽæŠŠpad_token_idçš„åœ°æ–¹æ¢æˆäº†-100ï¼Œä½œä¸ºæ ‡ç­¾labelsï¼Œç­‰ç­‰ï¼Œé‚£æˆ‘å²‚ä¸æ˜¯å¯ä»¥ç›´æŽ¥ç”¨å‰é¢å®šä¹‰çš„PreCollaotrï¼Ÿï¼ˆç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼ï¼‰

- mlmä»»åŠ¡å°±æ˜¯å¤šè°ƒç”¨äº†DataCollatorForLanguageModelingä¸­çš„`torch_mask_tokens`æ–¹æ³•ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ•´ä½“çš„é€»è¾‘ï¼š

![datacollatorforlanguagemodeling.drawio]((/assets/img/blog_imgs/2024-11-24-hf_datacollator/datacollatorforlanguagemodeling.drawio.png){: .mx-auto.d-block :}

{: .box-note}
**Note:** æ€»è€Œè¨€ä¹‹ï¼Œä¹Ÿå°±æ˜¯åœ¨å®šä¹‰PreCollatorçš„æ—¶å€™å¤šå®šä¹‰å‡ ä¸ªå…³é”®çš„å±žæ€§å’Œæ–¹æ³•ï¼Œç›´æŽ¥çœ‹ä»£ç å§ ~

```python
from typing import Union, List, Optional
from transformers import PreTrainedTokenizerBase, DataCollatorForLanguageModeling

vocab = {
    "[PAD]": 0,
    "[UNK]": 1,
    "[CLS]": 2,
    "[SEP]": 3,
    "[MASK]": 4,
    # ...
}

class PreCollator(PreTrainedTokenizerBase):

    padding_side = "right"
    all_special_ids = [0, 1, 2, 3, 4]

    def get_special_tokens_mask(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False
    ) -> List[int]:
        """
        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.
        Args:
            token_ids_0 (:obj:`List[int]`):
                List of ids of the first sequence.
            token_ids_1 (:obj:`List[int]`, `optional`):
                List of ids of the second sequence.
            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):
                Whether or not the token list is already formatted with special tokens for the model.
        Returns:
            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.
        """
        assert already_has_special_tokens and token_ids_1 is None, (
            "You cannot use ``already_has_special_tokens=False`` with this tokenizer. "
            "Please use a slow (full python) tokenizer to activate this argument."
            "Or set `return_special_tokens_mask=True` when calling the encoding method "
            "to get the special tokens mask in any tokenizer. "
        )

        all_special_ids = self.all_special_ids  # cache the property

        special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]


        return special_tokens_mask


    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:
        """
        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
        vocabulary.

        Args:
            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).

        Returns:
            `int` or `List[int]`: The token id or list of token ids.
        """
        if tokens is None:
            return None

        if isinstance(tokens, str):
            return self._convert_token_to_id_with_added_voc(tokens)

        ids = []
        for token in tokens:
            ids.append(self._convert_token_to_id_with_added_voc(token))
        return ids
    

    def _convert_token_to_id_with_added_voc(self, token):
        if token is None:
            return None

        return vocab.get(token)
    

    def __len__(self):
        return len(vocab)
    

precollator = PreCollator(pad_token="[PAD]", mask_token="[MASK]")
data_collator = DataCollatorForLanguageModeling(precollator, mlm=True, mlm_probability=0.15)


samples = [{"input_ids": [2, 10, 11, 12, 13, 3], "token_type_ids": [0, 0, 0, 0, 0, 0], "attention_mask": [1, 1, 1, 1, 1, 1]},
           {"input_ids": [2, 10, 11, 12, 13, 14, 3], "token_type_ids": [0, 0, 0, 0, 0, 0, 0], "attention_mask": [1, 1, 1, 1, 1, 1, 1]}]

data_collator(samples)
```

