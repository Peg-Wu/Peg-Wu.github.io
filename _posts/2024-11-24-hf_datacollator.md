---
layout: post
title: Huggingface DataCollator
subtitle: ğŸ˜ æ¬²ä¹°æ¡‚èŠ±åŒè½½é…’ï¼Œç»ˆä¸ä¼¼ï¼Œå°‘å¹´æ¸¸ã€‚
tags: [huggingface, datacollator]
gh-repo: Peg-Wu
gh-badge: [star, fork, follow]
cover-img: /assets/img/blog_imgs/2024-11-24-hf_datacollator/cover.png
comments: true
mathjax: true
author: Pengpeng Wu
---

{: .box-success}
æœ€è¿‘é‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œåšnlpçš„æ—¶å€™å¾ˆå¤štokenizeréƒ½æ˜¯ç›´æ¥ä»huggingface hubä¸Šæ‹‰ä¸‹æ¥çš„æˆ–è€…ç”¨è‡ªå·±çš„æ–‡æœ¬è¯­æ–™è®­ç»ƒå‡ºæ¥çš„ã€‚ä½†æ˜¯åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸï¼Œä»¥å•ç»†èƒè½¬å½•ç»„å¤§æ¨¡å‹geneformerä¸ºä¾‹ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªç»†èƒï¼Œæ¯ä¸ªtokenæ˜¯ä¸€ä¸ªgeneï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦è¿›è¡Œåˆ†è¯æ“ä½œï¼ˆç›¸å½“äºå·²ç»åˆ†å¥½è¯äº†ï¼‰ï¼Œæˆ‘ä»¬å®Œå…¨å¯ä»¥è‡ªå·±å®šä¹‰ä¸€ä¸ªTokenizerç±»ï¼Œç”¨äºå°†geneè½¬æ¢æˆidï¼Œå¹¶è¿›è¡Œæˆªæ–­ï¼Œå¡«å……ï¼ŒåŠ ç‰¹æ®Šå­—ç¬¦ç­‰æ“ä½œã€‚ç”±äºæˆ‘åç»­æƒ³è¦ä½¿ç”¨transformersä¸­çš„DataCollatorForLanguageModelingï¼Œä½†æ˜¯å®ä¾‹åŒ–è¿™ä¸ªç±»æ—¶å¿…é¡»è¦ä¼ å…¥ä¸€ä¸ªtokenizerï¼Œå¦‚æœæˆ‘æƒ³è¦ä½¿ç”¨è¿™ä¸ªdatacollatorï¼Œæˆ‘å¿…é¡»ææ¸…æ¥šå®ƒä½¿ç”¨äº†tokenizerä¸­çš„å“ªäº›å±æ€§å’Œæ–¹æ³•ï¼Œç„¶åè‡ªå®šä¹‰ä¸€ä¸ª"ä¼ª"tokenizeræˆ–è€…å°†å¿…è¦çš„å±æ€§å’Œæ–¹æ³•èå…¥æˆ‘è‡ªå®šä¹‰çš„Tokenizerç±»ï¼Œä½¿ä¸¤è€…èƒ½å¤Ÿç›¸äº’é€‚é…~ï¼ˆä¸»è¦æ˜¯å› ä¸ºå¤ªæ‡’äº†ï¼Œä¸æƒ³è‡ªå·±é‡æ–°å†™ä¸€ä¸ªdatacollatorï¼Œè€Œä¸”è¿™æ ·åšæ•ˆç‡ä¸ä¸€å®šæ¯”å®˜æ–¹å®ç°çš„é«˜ï¼Œä¸ä¼˜é›…ï¼‰ğŸ¤·â€â™‚ï¸

**åé¢æˆ‘å°±æ¢ç´¢äº†ä¸€ä¸‹DataCollatorWithPaddingå’ŒDataCollatorForLanguageModelingï¼Œè¿™ä¸¤ä¸ªä¹Ÿæ˜¯å¹³æ—¶æœ€å¸¸ä½¿ç”¨çš„ä¸¤ä¸ªdata_collator**

- æ•°æ®é›†ä¸‹è½½é“¾æ¥ï¼š[download_dataset](https://github.com/zyds/transformers-code/tree/master/02-NLP%20Tasks/14-language_model/wiki_cn_filtered)

- DEBUGï¼š

```python
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HOME"] = "~/hf"

from datasets import load_from_disk
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling


# load and preprocess dataset
ds = load_from_disk("./data/wiki_cn_filtered/")
tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-macbert-base")

def process_func(examples):
    return tokenizer(examples["completion"], max_length=384, truncation=True)

tokenized_ds = ds.map(process_func, batched=True, remove_columns=ds.column_names)


# dataloader, using different data_collator
dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorWithPadding(tokenizer))
# dl = DataLoader(tokenized_ds, batch_size=2, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=0.15))

next(iter(dl))
```

## 1. DataCollatorWithPadding

![datacollatorwithpadding.drawio](/assets/img/blog_imgs/2024-11-24-hf_datacollator/datacollatorwithpadding.drawio.png){: .mx-auto.d-block :}

{: .box-note}
**Note:** æ ¹æ®ä»¥ä¸Šæºç é€»è¾‘ï¼Œæˆ‘ä»¬ä¼¼ä¹åªéœ€è¦ç»§æ‰¿`PretrainedTokenizerBase`ç±»ï¼Œè‡ªå®šä¹‰å¿…è¦å±æ€§å³å¯ï¼Œå…·ä½“çš„å‡½æ•°å®ç°æ— éœ€ä¿®æ”¹ï¼Œå…¶ä¸­ï¼š`pad_token`å’Œ`pad_token_id`å±æ€§æ˜¯å¿…é¡»è¦æ·»åŠ çš„ï¼Œ`pad_token`å¯ä»¥é€šè¿‡è°ƒç”¨`SpecialTokensMixin`çš„initæ–¹æ³•å¸®åŠ©æˆ‘ä»¬æ·»åŠ ï¼Œ`pad_token_id`å¯ä»¥é€šè¿‡å®šä¹‰`convert_tokens_to_ids`æ–¹æ³•éšå¼æ·»åŠ ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨ç±»å±æ€§ä¸­è®¾ç½®`padding_side`ï¼Œç¡®å®šå¡«å……çš„ä½ç½®ï¼Œè¯ä¸å¤šè¯´ï¼Œç›´æ¥ä¸Šä»£ç ğŸ¥±~

### 1.1 æš´åŠ›ä¿®æ”¹

```python
from transformers import PreTrainedTokenizerBase, DataCollatorWithPadding

class PreCollator(PreTrainedTokenizerBase):
    pad_token = ["PAD"]
    pad_token_id = 0
    padding_side = "right"

precollator = PreCollator()
data_collator = DataCollatorWithPadding(precollator)


samples = [{"input_ids": [1, 2, 3], "token_type_ids": [0, 0, 0], "attention_mask": [1, 1, 1], "labels": 1},
           {"input_ids": [4, 5], "token_type_ids": [0, 0], "attention_mask": [1, 1], "labels": 0}]

data_collator(samples)
```

### 1.2 æ›´åŠ ä¼˜é›…çš„æ–¹å¼

```python
from typing import Union, List
from transformers import PreTrainedTokenizerBase, DataCollatorWithPadding

vocab = {
    "[PAD]": 0,
    "[UNK]": 1,
    "[CLS]": 2,
    "[SEP]": 3,
    "[MASK]": 4,
    # ...
}

class PreCollator(PreTrainedTokenizerBase):

    padding_side = "right"

    def convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]:
        """
        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
        vocabulary.

        Args:
            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).

        Returns:
            `int` or `List[int]`: The token id or list of token ids.
        """
        if tokens is None:
            return None

        if isinstance(tokens, str):
            return self._convert_token_to_id_with_added_voc(tokens)

        ids = []
        for token in tokens:
            ids.append(self._convert_token_to_id_with_added_voc(token))
        return ids
    

    def _convert_token_to_id_with_added_voc(self, token):
        if token is None:
            return None

        return vocab.get(token)
    

precollator = PreCollator(pad_token="[PAD]")
data_collator = DataCollatorWithPadding(precollator)


samples = [{"input_ids": [1, 2, 3], "token_type_ids": [0, 0, 0], "attention_mask": [1, 1, 1], "labels": 1},
           {"input_ids": [4, 5], "token_type_ids": [0, 0], "attention_mask": [1, 1], "labels": 0}]

data_collator(samples)
```

ğŸ˜‚ è™½ç„¶ä¼˜é›…ï¼Œä½†æ˜¯éœ€è¦å¯¹æºç æœ‰å……åˆ†çš„ç†è§£ ~

## 2. DataColloatorForLanguageModeling

- å®Œå–„ä¸­......
