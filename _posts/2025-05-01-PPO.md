---
layout: post
title: PPO
subtitle: ğŸ¤– RLHF
tags: [huggingface, RL]
gh-repo: Peg-Wu
gh-badge: [star, fork, follow]
cover-img: /assets/img/blog_imgs/2025-05-01-PPO/cover.png
comments: true
mathjax: true
author: Pengpeng Wu
---

{: .box-success}
ğŸˆ æ¥ä¸Šç¯‡ï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨ç»å†SFTå’Œè®­ç»ƒReward Modelåï¼Œåé¢ä¸€ä¸ªé˜¶æ®µå°±æ˜¯å¼€å§‹PPOå¼ºåŒ–å­¦ä¹ ï¼Œè¿™æ ·å°±å®Œæˆäº†æ•´ä¸ªRLHFè¿‡ç¨‹ã€‚æœ¬èŠ‚å°†ä»å·¥ç¨‹åŒ–è§†è§’è§£è¯»trlæ˜¯å¦‚ä½•å®ç°PPOå¼ºåŒ–å­¦ä¹ çš„~

- å‚è€ƒä»£ç ï¼šhttps://github.com/huggingface/trl/blob/main/examples/scripts/ppo/ppo.py
- transformersï¼š4.49.0; trl: 0.17.0

## 1. Dataset

- æ•°æ®é›†æ–¹é¢ï¼Œæˆ‘ä»¬åªéœ€è¦å‡†å¤‡Queryå³å¯ï¼Œä¸éœ€è¦å®Œæ•´çš„é—®ç­”æ•°æ®

## 2. Model

- æˆ‘ä»¬ä¸€å…±éœ€è¦å››ä¸ªæ¨¡å‹:
  - Policy Model: ä½¿ç”¨SFTåçš„æ¨¡å‹åˆå§‹åŒ–ï¼Œéœ€è¦æ›´æ–°å‚æ•°
  - Reference Policy Model: ä½¿ç”¨SFTåçš„æ¨¡å‹åˆå§‹åŒ–ï¼Œä¸éœ€è¦æ›´æ–°å‚æ•°
  - Reward Model: ä½¿ç”¨Reward Modelåˆå§‹åŒ–ï¼Œä¸éœ€è¦æ›´æ–°å‚æ•°
  - Value Model: ä½¿ç”¨Reward Modelåˆå§‹åŒ–ï¼Œéœ€è¦æ›´æ–°å‚æ•°

## 3. Workflow

- æ³¨æ„ï¼šåœ¨æ­£å¼è¿›å…¥PPOç®—æ³•è®­ç»ƒä¹‹å‰ï¼Œä»¥ä¸‹çš„æ‰€æœ‰çš„æ­¥éª¤éƒ½æ˜¯ä¸è¦æ›´æ–°æ¢¯åº¦çš„ï¼

### 3.1 Step 1

![image-1](/assets/img/blog_imgs/2025-05-01-PPO/1.png)

- é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä»Datasetä¸­å–å‡ºä¸€ä¸ªbatchçš„queryï¼Œå°†å…¶ä¼ å…¥Policy Modelï¼Œå¾—åˆ°ç›¸åº”çš„responseå’Œlogitsï¼Œè¿™ä¸ªè¿‡ç¨‹å°±æ˜¯åœ¨è®©Policy Modelæ ¹æ®queryç”Ÿæˆå›ç­”ï¼Œç”Ÿæˆå›ç­”çš„æœ€å¤§é•¿åº¦æˆ‘ä»¬éœ€è¦åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šå¥½ï¼Œç”Ÿæˆçš„å›ç­”å¦‚æœæ²¡æœ‰è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼ˆå³ï¼šé‡åˆ°eos tokenï¼Œæå‰ç»ˆæ­¢ç”Ÿæˆï¼‰ï¼Œæˆ‘ä»¬ç”¨pad tokenå¡«å……ï¼Œlogitsæˆ‘ä»¬ç”¨0å¡«å……ï¼›å¾—åˆ°responseå’Œlogitsåï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—logprob
- **<u>ä½†æ˜¯ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œé…ç½®æ–‡ä»¶ä¸­è®¾ç½®äº†early_stopping=Falseï¼Œå› æ­¤ï¼Œå³ä½¿é‡åˆ°äº†ç»ˆæ­¢ç¬¦ï¼Œè¿˜æ˜¯ä¼šç»§ç»­ç”Ÿæˆåˆ°æœ€å¤§é•¿åº¦ï¼Œå¹¶æ²¡æœ‰è¿›è¡Œå¡«å……</u>**
- æ¥ç€ï¼Œæˆ‘ä»¬éœ€è¦å°†queryå’Œresponseæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä¼ å…¥Reference Policy Modelï¼Œè¾“å‡ºref_logitsï¼Œæˆ‘ä»¬å–responseéƒ¨åˆ†çš„logitsï¼Œé™¤ä»¥ä¸€ä¸ªtemperatureï¼ˆdefault: 0.7ï¼‰ç³»æ•°ï¼Œç„¶åè”åˆresponseï¼Œè®¡ç®—ref_logprob
- å…·ä½“è®¡ç®—log_probå’Œref_logprobçš„å‡½æ•°å¦‚ä¸‹ï¼š

```python
def selective_log_softmax(logits, index):
    """
    A memory-efficient implementation of the common `log_softmax -> gather` operation.

    This function is equivalent to the following naive implementation:
    ```python
    logps = torch.gather(logits.log_softmax(-1), dim=-1, index=index.unsqueeze(-1)).squeeze(-1)

    Args:
        logits (`torch.Tensor`):
            Logits tensor of shape `(..., num_classes)`.
        index (`torch.Tensor`):
            Index tensor of shape `(...)`, specifying the positions to gather from the log-softmax output.
    
    Returns:
        `torch.Tensor`:
            Gathered log probabilities with the same shape as `index`.
    """
    if logits.dtype in [torch.float32, torch.float64]:
        selected_logits = torch.gather(logits, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)
        # loop to reduce peak mem consumption
        logsumexp_values = torch.stack([torch.logsumexp(lg, dim=-1) for lg in logits])
        per_token_logps = selected_logits - logsumexp_values  # log_softmax(x_i) = x_i - logsumexp(x)
    else:
        # logsumexp approach is unstable with bfloat16, fall back to slightly less efficent approach
        per_token_logps = []
        for row_logits, row_labels in zip(logits, index):  # loop to reduce peak mem consumption
            row_logps = F.log_softmax(row_logits, dim=-1)
            row_per_token_logps = row_logps.gather(dim=-1, index=row_labels.unsqueeze(-1)).squeeze(-1)
            per_token_logps.append(row_per_token_logps)
        per_token_logps = torch.stack(per_token_logps)
    return per_token_logps
```

### 3.2 Step 2

![image-2](/assets/img/blog_imgs/2025-05-01-PPO/2.png)

- Truncateæ“ä½œæ˜¯ä¸ºäº†åº”å¯¹ä¸€äº›è¾¹ç¼˜æƒ…å†µï¼Œæ­¤å¤„æˆ‘æš‚æ—¶æ²¡æœ‰çœ‹æ‡‚ï¼Œç¤ºä¾‹ä»£ç ä¸­response = process_responseï¼Œå¹¶æ²¡æœ‰è¿›è¡ŒTruncateè¿™ä¸€æ“ä½œï¼Œæ­¤å¤„å…ˆè¿™æ ·ç®€å•ç†è§£
- å°†queryå’Œresponseæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä¼ å…¥Value Modelï¼Œå¾—åˆ°valueï¼Œå–responseéƒ¨åˆ†çš„valueï¼Œå³å¯å¾—åˆ°æœ€ç»ˆçš„value
- å°†queryå’Œprocess_responseæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä¼ å…¥Reward Modelï¼Œå¾—åˆ°scoreï¼Œç†è®ºä¸Šï¼Œæˆ‘ä»¬å¾—åˆ°çš„scoreå½¢çŠ¶åº”è¯¥æ˜¯ï¼ˆBï¼ŒLq + Lrï¼‰ï¼Œè¿™é‡Œçš„å½¢çŠ¶æ˜¯ï¼ˆBï¼Œï¼‰ï¼Œå› ä¸ºæ¯ä¸ªæ ·æœ¬æˆ‘ä»¬å–çš„æ˜¯æœ€åä¸€ä¸ªépad tokençš„logitsï¼›ç”±äºæ²¡æœ‰Truncateå’Œpadæ“ä½œï¼Œå› æ­¤å–çš„å°±æ˜¯æœ€åä¸€ä¸ªtokençš„logits
- valueå’Œscoreçš„è®¡ç®—ç”¨çš„æ˜¯åŒä¸€ä¸ªå‡½æ•°ï¼š


```python
def get_reward(
    model: torch.nn.Module, query_responses: torch.Tensor, pad_token_id: int, context_length: int
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Computes the reward logits and the rewards for a given model and query responses.

    Args:
        model (`torch.nn.Module`):
            The model used to compute the reward logits.
        query_responses (`torch.Tensor`):
            The tensor containing the query responses.
        pad_token_id (`int`):
            The token ID representing the pad token.
        context_length (`int`):
            The length of the context in the query responses.

    Returns:
        tuple:
            - `reward_logits` (`torch.Tensor`):
                The logits for the reward model.
            - `final_rewards` (`torch.Tensor`):
                The final rewards for each query response.
            - `sequence_lengths` (`torch.Tensor`):
                The lengths of the sequences in the query responses.
    """
    attention_mask = query_responses != pad_token_id
    position_ids = attention_mask.cumsum(1) - attention_mask.long()  # exclusive cumsum
    lm_backbone = getattr(model, model.base_model_prefix)
    input_ids = torch.masked_fill(query_responses, ~attention_mask, 0)
    output = lm_backbone(
        input_ids=input_ids,
        attention_mask=attention_mask,
        position_ids=position_ids,
        return_dict=True,
        output_hidden_states=True,
        use_cache=False,  # otherwise mistral-based RM would error out
    )
    reward_logits = model.score(output.hidden_states[-1])
    sequence_lengths = first_true_indices(query_responses[:, context_length:] == pad_token_id) - 1 + context_length
    # https://github.com/huggingface/transformers/blob/dc68a39c8111217683bf49a4912d0c9018bab33d/src/transformers/models/gpt2/modeling_gpt2.py#L1454
    return (
        reward_logits,  # æ‰€æœ‰tokençš„logits, å½¢çŠ¶ä¸º(batch_size, qa_len, 1)
        reward_logits[
            torch.arange(reward_logits.size(0), device=reward_logits.device),
            sequence_lengths,
        ].squeeze(-1),  # å–æœ€åä¸€ä¸ªépad tokençš„logits, å½¢çŠ¶ä¸º(batch_size,)
        sequence_lengths,  # æ¯ä¸ªqa, æœ€åä¸€ä¸ªépad tokençš„index, å½¢çŠ¶ä¸º(batch_size,)
    )
```

### 3.3 Step 3

![image-3](/assets/img/blog_imgs/2025-05-01-PPO/3.png)

- Step 2ä¸­æˆ‘ä»¬å¾—åˆ°äº†Reward Modeläº§ç”Ÿçš„scoreï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ ¹æ®æ¯ä¸ªæ ·æœ¬æ˜¯å¦åŒ…å«eos tokenå¯¹åˆ†æ•°è¿›ä¸€æ­¥è¿›è¡Œè°ƒæ•´ï¼Œå¦‚æœä¸åŒ…å«eos tokenï¼Œæˆ‘ä»¬éœ€è¦å¯¹ç›¸åº”æ ·æœ¬çš„scoreç½šåˆ†ï¼Œé»˜è®¤ç½š1åˆ†
- æ¥ä¸‹æ¥æ˜¯ä¸€äº›æ©ç çš„æ“ä½œï¼š
  - logprobå’Œref_logprobéœ€è¦å¯¹pad tokençš„ä½ç½®è¿›è¡Œæ©ç ï¼Œå¹¶æ›¿æ¢æˆINVALID_LOGPROB: 1.0
  - valueéœ€è¦å¯¹pad tokençš„ä½ç½®è¿›è¡Œæ©ç ï¼Œå¹¶æ›¿æ¢æˆ0ï¼Œå¹¶ä¸”ç¬¬ä¸€ä¸ªpad tokençš„ä½ç½®ä¸ä¼šè¢«æ©ç 
  - è¿™é‡Œä¹Ÿæ˜¯åªæœ‰è¿›è¡Œäº†Truncateæ“ä½œï¼Œè¿™é‡Œçš„æ©ç æ‰ä¼šç”Ÿæ•ˆï¼Œå› ä¸ºä»–æ˜¯æ ¹æ®process_responseä¸­pad tokençš„ä½ç½®æ¥è¿›è¡Œæ©ç çš„

### 3.4 Step 4

![image-4](/assets/img/blog_imgs/2025-05-01-PPO/4.png)

- æœ€åï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®logprobå’Œref_logprobè®¡ç®—klæ•£åº¦ï¼Œç„¶åä¸scoreç›¸åŠ å¾—åˆ°rewards
- åŒæ ·çš„ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ ¹æ®rewardså’Œvalueè®¡ç®—å¾—åˆ°advantageså’Œreturns
- è¿™é‡Œå°±æ˜¯ä¸€äº›æ•°å­¦ä¸Šçš„æ“ä½œï¼Œç›¸å…³çš„ä»£ç å¦‚ä¸‹ï¼š(è¯¦ç»†è¿‡ç¨‹è¯·è¿›å…¥æºç æŸ¥çœ‹)

```python
def masked_whiten(values: torch.Tensor, mask: torch.Tensor, shift_mean: bool = True) -> torch.Tensor:
    """Whiten values with masked values."""
    mean, var = masked_mean(values, mask), masked_var(values, mask)
    whitened = (values - mean) * torch.rsqrt(var + 1e-8)
    if not shift_mean:
        whitened += mean
    return whitened

# 4. compute rewards
# Formula used by http://joschu.net/blog/kl-approx.html for the k1 and k3 estimators
logr = ref_logprobs - logprobs
kl = -logr if args.kl_estimator == "k1" else (logr.exp() - 1) - logr  # Else statement is k3
non_score_reward = -args.kl_coef * kl
rewards = non_score_reward.clone()
actual_start = torch.arange(rewards.size(0), device=rewards.device)
actual_end = torch.where(sequence_lengths_p1 < rewards.size(1), sequence_lengths_p1, sequence_lengths)
rewards[[actual_start, actual_end]] += scores

# 5. whiten rewards
if args.whiten_rewards:
    rewards = masked_whiten(rewards, mask=~padding_mask_p1, shift_mean=False)
    rewards = torch.masked_fill(rewards, padding_mask_p1, 0)

# 6. compute advantages and returns
lastgaelam = 0
advantages_reversed = []
gen_length = responses.shape[1]
for t in reversed(range(gen_length)):
    nextvalues = values[:, t + 1] if t < gen_length - 1 else 0.0
    delta = rewards[:, t] + args.gamma * nextvalues - values[:, t]
    lastgaelam = delta + args.gamma * args.lam * lastgaelam
    advantages_reversed.append(lastgaelam)
advantages = torch.stack(advantages_reversed[::-1], axis=1)
returns = advantages + values
advantages = masked_whiten(advantages, ~padding_mask)
advantages = torch.masked_fill(advantages, padding_mask, 0)
torch.cuda.empty_cache()
```

> è‡³æ­¤ï¼Œæ‰€æœ‰çš„æ•°æ®å‡†å¤‡é˜¶æ®µå°±å®Œæˆäº†ï¼Œåé¢å°±æ­£å¼è¿›å…¥äº†PPOä¼˜åŒ–è¿‡ç¨‹ï¼Œå°±æ˜¯ä¸€äº›è®¡ç®—lossï¼Œåå‘ä¼ æ’­çš„è¿‡ç¨‹~


- ç”±äºæœ¬äººæ˜¯åšç”Ÿç‰©ä¿¡æ¯å­¦çš„ï¼Œå¹³æ—¶åªç²—æµ…çš„äº†è§£è¿‡ä¸€äº›å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬ç†è®ºï¼Œå­¦ä¹ PPOçš„ä»£ç ä¹Ÿåªæ˜¯æƒ³äº†è§£ä¸€ä¸‹ä»–æ•´ä¸ªçš„æ•°æ®æµå‘æ˜¯å¦‚ä½•è¿›è¡Œçš„ï¼Œå¦‚æœå¯¹ä»£ç è§£è¯»æœ‰è¯¯ï¼Œè¿˜è¯·å¤§å®¶å¤šå¤šæŒ‡æ­£~ğŸ˜Š
- æœ€åï¼Œé™„ä¸Šæ•´ä¸ªæµç¨‹çš„ç®€æ˜ç¤ºæ„å›¾ï¼š

![image-5](/assets/img/blog_imgs/2025-05-01-PPO/5.png)